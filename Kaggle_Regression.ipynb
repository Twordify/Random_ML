{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATVrII03HYHm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import ensemble, tree, linear_model\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
        "from sklearn.svm import SVR\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "print(f'Train size: {train.shape}')\n",
        "print(f'Test size: {test.shape}')"
      ],
      "metadata": {
        "id": "5ziE6JZFH-zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA**"
      ],
      "metadata": {
        "id": "rb44p1-GNqVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if any feature has high correlation with price\n",
        "\n",
        "corr = train.corr()\n",
        "high_corr = corr.index[abs(corr['SalePrice'])> 0.5]\n",
        "plt.figure(figsize=(12,12))\n",
        "corr_plot = sns.heatmap(train[high_corr].corr(), annot=True)"
      ],
      "metadata": {
        "id": "Qb-o-ywGIDb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OveralQual has correlation = 0.79. Plot this feature\n",
        "\n",
        "sns.barplot(x = 'OverallQual', y = 'SalePrice', data = train)"
      ],
      "metadata": {
        "id": "VahMNBbCJPGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check other features with high correlation\n",
        "\n",
        "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "#sns.pairplot(vars = cols, data = train)"
      ],
      "metadata": {
        "id": "oYWONF7WKSV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats \n",
        "from scipy.stats import norm, skew\n",
        "\n",
        "sns.distplot(train['SalePrice'], fit = norm)\n",
        "\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TPB0RE0oLDCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform to reduce skewed distplot\n",
        "\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
      ],
      "metadata": {
        "id": "ArYjIu5JR3gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show transformed distplot\n",
        "\n",
        "sns.distplot(train['SalePrice'], fit = norm)\n",
        "(mu, sigma) = norm.fit(train['SalePrice'])\n",
        "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
        "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('SalePrice distribution')\n",
        "\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train['SalePrice'], plot=plt)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SFOES_SWSV4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "xq9KzSwCNudJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)"
      ],
      "metadata": {
        "id": "Aw3mBfb6N7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove outliers\n",
        "train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n",
        "train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n",
        "train.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "7ha1O4TkPGVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split features\n",
        "train_labels = train['SalePrice'].reset_index(drop=True)\n",
        "train_features = train.drop(['SalePrice'], axis=1)\n",
        "test_features = test\n",
        "\n",
        "#Remove index\n",
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "train.drop(['Id'], axis=1, inplace=True)\n",
        "test.drop(['Id'], axis=1, inplace=True)\n",
        "\n",
        "#Concat train and test features\n",
        "all_features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
        "#all_features.shape\n"
      ],
      "metadata": {
        "id": "E76eisG4PkbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling missing values**"
      ],
      "metadata": {
        "id": "66ReGDl0S2Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Show where data is missing\n",
        "data_na = all_features.isnull().sum()\n",
        "data_na = data_na[data_na > 0]\n",
        "data_na.sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "LNGu0StMRaDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert non-numeric predictors to strings\n",
        "\n",
        "all_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\n",
        "all_features['YrSold'] = all_features['YrSold'].astype(str)\n",
        "all_features['MoSold'] = all_features['MoSold'].astype(str)\n"
      ],
      "metadata": {
        "id": "3qMJLQBgUXPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Def function to handle missing values (based on dataset's documentation)\n",
        "\n",
        "def handle_missing(features):\n",
        "    #Replace NA with 'Typ' what refers to 'Typical Functionality'\n",
        "    features['Functional'] = features['Functional'].fillna('Typ')\n",
        "    #Replace missing values with their mode\n",
        "    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n",
        "    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n",
        "    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n",
        "    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
        "    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
        "    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
        "    #Replace NA (based on documentation whre NA means \"No Pool\")\n",
        "    features['PoolQC'] = features['PoolQC'].fillna('None')\n",
        "    #Replace missing values with 0 in columns where there is no garage\n",
        "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
        "      features[col] = features[col].fillna(0)\n",
        "    #Replace missing values with None\n",
        "    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
        "        features[col] = features[col].fillna('None')\n",
        "    #Replace NA in basement related features as there is no basement\n",
        "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
        "        features[col] = features[col].fillna('None')\n",
        "    #Group by neighborhoods and fill with median\n",
        "    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "    #Fill rest of columns with None or 0\n",
        "    objects = []\n",
        "    for i in features.columns:\n",
        "        if features[i].dtype == object:\n",
        "            objects.append(i)\n",
        "    features.update(features[objects].fillna('None'))\n",
        "\n",
        "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    numeric = []\n",
        "    for i in features.columns:\n",
        "        if features[i].dtype in numeric_dtypes:\n",
        "            numeric.append(i)\n",
        "    features.update(features[numeric].fillna(0))    \n",
        "    return features\n",
        "\n",
        "all_features = handle_missing(all_features)"
      ],
      "metadata": {
        "id": "gUi1x_FwVDA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dummies and delete duplicated columns\n",
        "all_features = pd.get_dummies(all_features).reset_index(drop=True)\n",
        "all_features.shape\n",
        "\n",
        "all_features = all_features.loc[:,~all_features.columns.duplicated()]"
      ],
      "metadata": {
        "id": "jahrPa66oZzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating model**"
      ],
      "metadata": {
        "id": "t62S-SfMqLL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = all_features.iloc[:len(train_labels), :]\n",
        "X_test = all_features.iloc[len(train_labels):, :]\n",
        "X.shape, train_labels.shape, X_test.shape"
      ],
      "metadata": {
        "id": "UWMO6NozZWH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=12, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "BV3axWXWA42_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define error metrics\n",
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "def cv_rmse(model, X=X):\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n",
        "    return (rmse)"
      ],
      "metadata": {
        "id": "ylE2guZoBKPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lightgbm = LGBMRegressor(objective='regression', \n",
        "                       num_leaves=6,\n",
        "                       learning_rate=0.01, \n",
        "                       n_estimators=7000,\n",
        "                       max_bin=200, \n",
        "                       bagging_fraction=0.8,\n",
        "                       bagging_freq=4, \n",
        "                       bagging_seed=8,\n",
        "                       feature_fraction=0.2,\n",
        "                       feature_fraction_seed=8,\n",
        "                       min_sum_hessian_in_leaf = 11,\n",
        "                       verbose=-1,\n",
        "                       random_state=42)\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.01,\n",
        "                       n_estimators=6000,\n",
        "                       max_depth=4,\n",
        "                       min_child_weight=0,\n",
        "                       gamma=0.6,\n",
        "                       subsample=0.7,\n",
        "                       colsample_bytree=0.7,\n",
        "                       objective='reg:linear',\n",
        "                       nthread=-1,\n",
        "                       scale_pos_weight=1,\n",
        "                       seed=27,\n",
        "                       reg_alpha=0.00006,\n",
        "                       random_state=42)\n",
        "\n",
        "# Ridge Regressor\n",
        "ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n",
        "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n",
        "\n",
        "# Support Vector Regressor\n",
        "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
        "                                learning_rate=0.01,\n",
        "                                max_depth=4,\n",
        "                                max_features='sqrt',\n",
        "                                min_samples_leaf=15,\n",
        "                                min_samples_split=10,\n",
        "                                loss='huber',\n",
        "                                random_state=42)  \n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=1200,\n",
        "                          max_depth=15,\n",
        "                          min_samples_split=5,\n",
        "                          min_samples_leaf=5,\n",
        "                          max_features=None,\n",
        "                          oob_score=True,\n",
        "                          random_state=42)\n",
        "\n",
        "# Stack up all the models above, optimized using xgboost\n",
        "stack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n",
        "                                meta_regressor=xgboost,\n",
        "                                use_features_in_secondary=True)"
      ],
      "metadata": {
        "id": "Bd2QPSZZCa9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {}\n",
        "\n",
        "score = cv_rmse(lightgbm)\n",
        "print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['lgb'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "tydygHchDl0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cv_rmse(xgboost)\n",
        "print(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['xgb'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "b50uO0INEB0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cv_rmse(svr)\n",
        "print(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['svr'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "iOZkUQsVEKQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cv_rmse(ridge)\n",
        "print(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['ridge'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "NlZwD_IVELKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cv_rmse(rf)\n",
        "print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['rf'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "bqie3ai2EPBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cv_rmse(gbr)\n",
        "print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
        "scores['gbr'] = (score.mean(), score.std())"
      ],
      "metadata": {
        "id": "apsn9HpwEQ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('stack_gen')\n",
        "stack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))\n",
        "\n",
        "print('lightgbm')\n",
        "lgb_model_full_data = lightgbm.fit(X, train_labels)\n",
        "\n",
        "print('xgboost')\n",
        "xgb_model_full_data = xgboost.fit(X, train_labels)\n",
        "\n",
        "print('Svr')\n",
        "svr_model_full_data = svr.fit(X, train_labels)\n",
        "\n",
        "print('Ridge')\n",
        "ridge_model_full_data = ridge.fit(X, train_labels)\n",
        "\n",
        "print('RandomForest')\n",
        "rf_model_full_data = rf.fit(X, train_labels)\n",
        "\n",
        "print('GradientBoosting')\n",
        "gbr_model_full_data = gbr.fit(X, train_labels)\n"
      ],
      "metadata": {
        "id": "3x-ZPeTrGB1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}